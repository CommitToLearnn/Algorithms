<div align="center">

<img width="100%" src="https://capsule-render.vercel.app/api?type=waving&color=4A90E2&height=120&section=header&text=Naive%20Bayes&fontSize=40&fontColor=fff&animation=twinkling&fontAlignY=40&desc=Classificador%20ProbabilÃ­stico%20para%20Machine%20Learning&descAlignY=65&descSize=16">

</div>

# Classificador Naive Bayes

## DescriÃ§Ã£o

Naive Bayes Ã© um algoritmo de classificaÃ§Ã£o probabilÃ­stica baseado no Teorema de Bayes com a suposiÃ§Ã£o "naive" (ingÃªnua) de independÃªncia condicional entre as features. Apesar da simplicidade, Ã© muito eficaz para muitas aplicaÃ§Ãµes reais, especialmente classificaÃ§Ã£o de texto.

## Como Funciona

O algoritmo calcula a probabilidade posterior de cada classe dada as features observadas:

```
P(classe|features) = P(features|classe) Ã— P(classe) / P(features)
```

### SuposiÃ§Ã£o de IndependÃªncia
```
P(xâ‚,xâ‚‚,...,xâ‚™|classe) = P(xâ‚|classe) Ã— P(xâ‚‚|classe) Ã— ... Ã— P(xâ‚™|classe)
```

## Teorema de Bayes

### FÃ³rmula Base
```
P(A|B) = P(B|A) Ã— P(A) / P(B)
```

### Para ClassificaÃ§Ã£o
```
P(Cáµ¢|X) = P(X|Cáµ¢) Ã— P(Cáµ¢) / P(X)
```

Onde:
- **P(Cáµ¢|X)**: Probabilidade posterior da classe i
- **P(X|Cáµ¢)**: Likelihood (verossimilhanÃ§a)
- **P(Cáµ¢)**: Probabilidade a priori da classe i
- **P(X)**: EvidÃªncia (normalizador)

## Tipos Implementados

### 1. Gaussian Naive Bayes
Para features contÃ­nuas (numÃ©ricas):
```python
P(xáµ¢|classe) = (1/âˆš(2Ï€ÏƒÂ²)) Ã— exp(-(xáµ¢-Î¼)Â²/(2ÏƒÂ²))
```

### 2. Multinomial Naive Bayes  
Para features categÃ³ricas:
```python
P(xáµ¢|classe) = (count(xáµ¢, classe) + Î±) / (count(classe) + Î±Ã—V)
```
Onde Î± = 1 (SuavizaÃ§Ã£o de Laplace)

## Complexidade

| OperaÃ§Ã£o | Complexidade | ObservaÃ§Ãµes |
|----------|--------------|-------------|
| **Treinamento** | O(nÃ—d) | n=amostras, d=features |
| **PrediÃ§Ã£o** | O(cÃ—d) | c=classes, d=features |
| **MemÃ³ria** | O(cÃ—d) | Armazena estatÃ­sticas |

## ImplementaÃ§Ã£o DisponÃ­vel

### Python (`naive_bayes_basico.py`)
```python
# Exemplo de uso
from naive_bayes_basico import NaiveBayesClassifier

# Criar classificador
nb = NaiveBayesClassifier()

# Dados de treino
X_train = [[5.1, 3.5, 1.4, 0.2], [6.7, 3.1, 4.4, 1.4]]
y_train = ['setosa', 'versicolor']

# Treinar modelo
nb.fit(X_train, y_train)

# Fazer prediÃ§Ã£o
sample = [5.0, 3.0, 1.5, 0.3]
predicted_class, confidence = nb.predict(sample)

print(f"Classe: {predicted_class}")
print(f"ConfianÃ§a: {confidence:.3f}")
```

## CaracterÃ­sticas da ImplementaÃ§Ã£o

### DetecÃ§Ã£o AutomÃ¡tica de Tipos
```python
def _determine_feature_type(self, values):
    # Detecta se feature Ã© categÃ³rica ou contÃ­nua
    unique_ratio = len(set(values)) / len(values)
    return 'continuous' if unique_ratio > 0.5 else 'categorical'
```

### SuavizaÃ§Ã£o de Laplace
```python
# Evita probabilidade zero para features nÃ£o vistas
likelihood = (count + 1) / (total_count + num_unique_values)
```

### CÃ¡lculo em Log-Space
```python
# Evita underflow numÃ©rico
log_prob = log(prior) + sum(log(likelihood_i) for i in features)
```

## Vantagens

### âœ… Pontos Fortes
- **Simplicidade**: FÃ¡cil de implementar e entender
- **Rapidez**: Treinamento e prediÃ§Ã£o muito rÃ¡pidos
- **Poucos Dados**: Funciona bem com datasets pequenos
- **Multiclasse**: Naturalmente suporta mÃºltiplas classes
- **Features Mistas**: Lida com categÃ³ricas e numÃ©ricas
- **Baseline**: Excelente como modelo de referÃªncia

### âŒ LimitaÃ§Ãµes
- **IndependÃªncia**: Assume independÃªncia entre features
- **Dados ContÃ­nuos**: Assume distribuiÃ§Ã£o Gaussiana
- **Features Correlacionadas**: Performance degrada
- **Desbalanceamento**: SensÃ­vel a classes desbalanceadas

## Casos de Uso

### 1. ClassificaÃ§Ã£o de Texto
```python
# AnÃ¡lise de sentimento
documents = [
    (["bom", "excelente", "Ã³timo"], "positivo"),
    (["ruim", "terrÃ­vel", "pÃ©ssimo"], "negativo")
]
```

### 2. Filtragem de Spam
```python
# ClassificaÃ§Ã£o de emails
features = ["viagra", "gratis", "dinheiro", "urgente"]
labels = ["spam", "ham"]
```

### 3. DiagnÃ³stico MÃ©dico
```python
# Baseado em sintomas
symptoms = ["febre", "tosse", "dor_cabeca"]
diagnosis = ["gripe", "covid", "resfriado"]
```

### 4. CategorizaÃ§Ã£o de NotÃ­cias
```python
# ClassificaÃ§Ã£o por tÃ³pico
topics = ["esportes", "politica", "tecnologia", "economia"]
```

## MÃ©tricas de AvaliaÃ§Ã£o

### ClassificaÃ§Ã£o BinÃ¡ria
```python
# PrecisÃ£o, Recall, F1-Score
precision = TP / (TP + FP)
recall = TP / (TP + FN)  
f1_score = 2 * (precision * recall) / (precision + recall)
```

### ClassificaÃ§Ã£o Multiclasse
```python
# AcurÃ¡cia macro e micro
macro_avg = mean([metric_per_class])
micro_avg = metric_across_all_samples
```

### Matriz de ConfusÃ£o
```
          Predito
Verdadeiro   A    B    C
    A       50    2    1
    B        3   45    2  
    C        1    1   48
```

## Exemplo Completo

### Dataset Iris (NumÃ©rico)
```python
# Features contÃ­nuas - usa Gaussian NB
X_iris = [
    [5.1, 3.5, 1.4, 0.2],  # sepal_length, sepal_width, petal_length, petal_width
    [6.7, 3.1, 4.4, 1.4],
    [6.3, 3.3, 6.0, 2.5]
]
y_iris = ['setosa', 'versicolor', 'virginica']

nb_iris = NaiveBayesClassifier()
nb_iris.fit(X_iris, y_iris)

# PrediÃ§Ã£o
sample = [5.0, 3.0, 1.5, 0.3]
result = nb_iris.predict(sample)
print(f"EspÃ©cie: {result[0]} (confianÃ§a: {result[1]:.3f})")
```

### Dataset Texto (CategÃ³rico)
```python
# Features categÃ³ricas - usa Multinomial NB
X_text = [
    ["bom", "excelente", "Ã³timo"],
    ["ruim", "terrÃ­vel", "pÃ©ssimo"],
    ["legal", "bacana", "interessante"]
]
y_text = ["positivo", "negativo", "positivo"]

nb_text = NaiveBayesClassifier()
nb_text.fit(X_text, y_text)

# PrediÃ§Ã£o
review = ["bom", "legal", "interessante"]
result = nb_text.predict(review)
print(f"Sentimento: {result[0]} (confianÃ§a: {result[1]:.3f})")
```

## OtimizaÃ§Ãµes AvanÃ§adas

### 1. Feature Selection
```python
# Seleciona features mais informativas
def select_features_by_chi2(X, y, k=10):
    # Implementa teste qui-quadrado
    pass
```

### 2. Balanceamento de Classes
```python
# Ajusta probabilidades a priori
def balanced_priors(y):
    unique_classes = set(y)
    return {cls: 1/len(unique_classes) for cls in unique_classes}
```

### 3. Ensemble Methods
```python
# Combina mÃºltiplos modelos Naive Bayes
def ensemble_prediction(models, sample):
    predictions = [model.predict_proba(sample) for model in models]
    return average_predictions(predictions)
```

## ComparaÃ§Ã£o com Outros Algoritmos

| Algoritmo | AcurÃ¡cia | Velocidade | Interpretabilidade |
|-----------|----------|------------|-------------------|
| **Naive Bayes** | ğŸ“Š Boa | ğŸš€ Muito RÃ¡pida | ğŸ‘ï¸ Alta |
| Logistic Regression | ğŸ“Š Boa | ğŸš€ RÃ¡pida | ğŸ‘ï¸ MÃ©dia |
| Random Forest | ğŸ“Š Muito Boa | ğŸš€ MÃ©dia | ğŸ‘ï¸ Baixa |
| SVM | ğŸ“Š Muito Boa | ğŸš€ Lenta | ğŸ‘ï¸ Baixa |
| Neural Networks | ğŸ“Š Excelente | ğŸš€ Muito Lenta | ğŸ‘ï¸ Muito Baixa |

## Variantes do Algoritmo

### 1. Gaussian Naive Bayes
- **Uso**: Features contÃ­nuas
- **DistribuiÃ§Ã£o**: Gaussiana (normal)
- **ParÃ¢metros**: Î¼ (mÃ©dia), ÏƒÂ² (variÃ¢ncia)

### 2. Multinomial Naive Bayes
- **Uso**: Contagem de features (texto)
- **DistribuiÃ§Ã£o**: Multinomial
- **ParÃ¢metros**: Probabilidades de cada termo

### 3. Bernoulli Naive Bayes
- **Uso**: Features binÃ¡rias
- **DistribuiÃ§Ã£o**: Bernoulli
- **ParÃ¢metros**: Probabilidade de ocorrÃªncia

### 4. Categorical Naive Bayes
- **Uso**: Features categÃ³ricas
- **DistribuiÃ§Ã£o**: CategÃ³rica
- **ParÃ¢metros**: Probabilidade de cada categoria

## Tratamento de Problemas

### Zero Probability
```python
# SuavizaÃ§Ã£o de Laplace
P(word|class) = (count(word, class) + Î±) / (count(class) + Î± * |V|)
```

### Numerical Underflow
```python
# Usar logaritmos
log_prob = log(prior) + sum(log(likelihood))
```

### Feature Scaling
```python
# Gaussian NB Ã© sensÃ­vel Ã  escala
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

## Executar Exemplo

```bash
cd python/naive_bayes
python naive_bayes_basico.py
```

## AplicaÃ§Ãµes Industriais

### 1. RecomendaÃ§Ã£o de Produtos
```python
# Baseado em histÃ³rico de compras
user_features = ["categoria_preferida", "faixa_preco", "marca"]
recommendation = nb.predict(user_features)
```

### 2. DetecÃ§Ã£o de Fraudes
```python
# Baseado em padrÃµes de transaÃ§Ã£o
transaction_features = ["valor", "horario", "local", "tipo"]
is_fraud = nb.predict(transaction_features)
```

### 3. ClassificaÃ§Ã£o de Documentos
```python
# OrganizaÃ§Ã£o automÃ¡tica de documentos
document_features = extract_text_features(document)
category = nb.predict(document_features)
```

## ReferÃªncias

- [Pattern Recognition and Machine Learning - Bishop](https://www.microsoft.com/en-us/research/people/cmbishop/)
- [The Elements of Statistical Learning - Hastie](https://web.stanford.edu/~hastie/ElemStatLearn/)
- [Machine Learning: A Probabilistic Perspective - Murphy](https://probml.github.io/pml-book/)
- [Naive Bayes and Text Classification - Manning](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html)

---

<div align="center">

## ğŸ‘¤ Autor | Author

<div align="center">
  <a href="https://github.com/matheussricardoo" target="_blank">
    <img src="https://skillicons.dev/icons?i=github" alt="GitHub"/>
  </a>
  <a href="https://www.linkedin.com/in/matheus-ricardo-426452266/" target="_blank">
    <img src="https://skillicons.dev/icons?i=linkedin" alt="LinkedIn"/>
  </a>
</div>

## ğŸ“„ LicenÃ§a | License

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo [LICENSE](../../LICENSE) para mais detalhes.

This project is licensed under the MIT License. See the [LICENSE](../../LICENSE) file for details.

---

<div align="center">
  <i>ğŸ’¡ "A simplicidade Ã© o Ãºltimo grau de sofisticaÃ§Ã£o" - Leonardo da Vinci</i>
  <br>
  <i>ğŸ’¡ "Simplicity is the ultimate sophistication" - Leonardo da Vinci</i>
</div>

<img width="100%" src="https://capsule-render.vercel.app/api?type=waving&color=4A90E2&height=120&section=footer"/>

</div>
